<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DevOps on Random Bits of Wisdom</title>
        <link>https://preslav.me/tags/devops/</link>
        <description>Recent content in DevOps on Random Bits of Wisdom</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Sun, 24 May 2020 10:28:44 +0100</lastBuildDate>
        <atom:link href="https://preslav.me/tags/devops/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>The Simplest Way to Keep a Go App Up and Running</title>
            <link>https://preslav.me/2020/05/24/the-simplest-way-to-keep-a-go-app-up-and-running/</link>
            <pubDate>Sun, 24 May 2020 10:28:44 +0100</pubDate>
            
            <guid>https://preslav.me/2020/05/24/the-simplest-way-to-keep-a-go-app-up-and-running/</guid>
            <description>Poor man’s Kubernetes</description>
            <content type="html"><![CDATA[<p>I am a lazy programmer, experimenting with new ideas all the time. For these, running and deploying tiny Go apps as single executables is the closest way to see something in action. Yet, quickly written Go apps often panic, and crash altogether. While I am a strong defender of making sure that all possible errors are checked, real software is bound to eventually crash, no matter what. In such situations, the fastest solution is usually a simple restart.</p>

<p><a href="https://www.freedesktop.org/wiki/Software/systemd/">Systemd</a>, <a href="https://www.docker.com/">Docker</a>, <a href="https://kubernetes.io/">Kubernetes</a> - all took process supervision to a new level. If you are working on production-level software, chances are you already have some infrastructure layer in place that will take care of this. On the other hand, setting up tons of infrastructure is the last thing you want to think about, when working on your quickly made side project. This is where the following 5-line Bash script comes in place:</p>

<pre><code class="language-bash">#!/bin/sh

while true; do
    .myapp
done

# call this file run.sh and use nohup to start it
# nohup ./run.sh &amp;
</code></pre>

<p>That’s it. Instead of running your app directly, you can use <code>nohup</code> to run it through the Bash script: <code>nohup run.sh &amp;</code>.</p>

<hr />

<p>NOTE: While fairly easy to set up, this is by no means a safe way to take care of process restarts. It is also not an excuse to avoid thorough error checking. It will help you see your app quickly, but as soon as it starts getting serious traffic, you need to consider setting up the proper infrastructure. I can <a href="https://preslav.me/contact/">help</a> with that.</p>
]]></content>
        </item>
        
        <item>
            <title>Securing your ElasticSearch instances</title>
            <link>https://preslav.me/2017/02/03/securing-your-elasticsearch-instances/</link>
            <pubDate>Fri, 03 Feb 2017 05:29:00 +0000</pubDate>
            
            <guid>https://preslav.me/2017/02/03/securing-your-elasticsearch-instances/</guid>
            <description>Securing your ElasticSearch instances and keeping all the fun Often, we choose convenience over security. Many modern tools such as MongoDB and ElasticSearch, have grown in popularity, partly because of their easy-to-set-up-and-tinker-with nature. Just spin off an instance, point your browser to the right URL and you&amp;rsquo;re ready to start sending queries.
Unfortunately, one thing comes for another, and as we have recently seen, ElasticSearch left in the open can be a vulnerable target, same as MongoDB was in its heyday.</description>
            <content type="html"><![CDATA[

<h3 id="securing-your-elasticsearch-instances-and-keeping-all-the-fun">Securing your ElasticSearch instances and keeping all the fun</h3>

<p>Often, we choose convenience over security. Many modern tools such as MongoDB and ElasticSearch, have grown in popularity, partly because of their easy-to-set-up-and-tinker-with nature. Just spin off an instance, point your browser to the right URL and you&rsquo;re ready to start sending queries.</p>

<p>Unfortunately, one thing comes for another, and as <a href="http://www.zdnet.com/article/elasticsearch-ransomware-attacks-now-number-in-the-thousands/">we have recently seen</a>, ElasticSearch left in the open can be a vulnerable target, same as MongoDB was in its heyday. In light of the <a href="http://www.zdnet.com/article/elasticsearch-ransomware-attacks-now-number-in-the-thousands/">recent attacks</a> on many open ElasticSearch instances across the world, I decided to share a quick tip on how to set remote ES instances, and keep them secure, by not compromising on its easy-to-play-with nature.</p>

<h2 id="part-one-restricting-the-access-to-your-elasticsearch-instance">Part One: Restricting the access to your ElasticSearch instance</h2>

<p>Let&rsquo;s start. The easiest way to setup an ElasticSearch instance is <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">spinning up a container</a> off the default Docker image:</p>

<pre><code class="language-bash">docker pull docker.elastic.co/elasticsearch/elasticsearch:&lt;VERSION&gt;

docker run -p 9200:9200 -e &quot;http.host=0.0.0.0&quot; -e &quot;transport.host=127.0.0.1&quot; elasticsearch:&lt;VERSION&gt;
</code></pre>

<p>Running the above line, will create a portion mapping from 9200 within the container, to port 9200 on the host machine. One problem here, is that by doing so, it also exposes it to the outside world. This could easily be seen by running <code>iptables</code> against your host:</p>

<pre><code class="language-bash">iptables -t nat -L -n

# Outputs
...
target     prot opt source               destination
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:9200 to:XXX.XXX.XXX.XXX:9200
...

</code></pre>

<p>Indeed, Docker takes the heavy-lifting of configuring your <code>iptables</code> firewall, but often, this may result in a configuration which is too permissive. What one should do instead, is provide a specific IP to the port mapping configuration. Thankfully, Docker supports this, so all we have to do is modify the above command, using the <code>IP:host_port:container_port</code> mapping:</p>

<pre><code class="language-bash">docker run -p 127.0.0.1:9200:9200 -e &quot;http.host=0.0.0.0&quot; -e &quot;transport.host=127.0.0.1&quot; elasticsearch:&lt;VERSION&gt;
</code></pre>

<p>Perfect! Putting the <code>127.0.0.1</code> will guarantee that the container will be available inside the host machine, but not accessible outside. A quick proof of this is looking at iptables again:</p>

<pre><code class="language-bash">target     prot opt source               destination
DNAT       tcp  --  0.0.0.0/0            127.0.0.1            tcp dpt:9200 to:XX.XXX.XXX.XXX:9200
</code></pre>

<p>if you point your browser to port 9200 you should not be able to see anything, but executing `curl 127.0.0.1:9200 from inside the host machine should work.</p>

<h2 id="part-two-accessing-your-elasticsearch-instance-in-a-secure-manner">Part Two: Accessing your ElasticSearch instance in a secure manner</h2>

<p>What we did was all fine, but how do access our ElasticSearch instance now, without losing the flexibility of quickly testing stuff on ES? Easy, using *NIX&rsquo;s Swiss Army Knife - <code>SSH</code>. SSH is a tool most programmers use on a daily basis, but fewer of them are aware that SSH allows for local and remote port forwarding. What this means is that SSH can create an encrypted tunnel between your machine and your server, such that you can accesses services running remotely, as if they were running on loclahost (local forwarding). There is also remote forwarding, which alternatively, allows you to securely access locally running services from your remote server.</p>

<p>While we are going to use local port forwarding in our case, both are analogous to each other:</p>

<pre><code class="language-bash">ssh -L/-R &lt;PORT_ON_THE_LOCAL/REMOTE_MACHINE&gt;:&lt;HOST_TO_MAP_TO&gt;:&lt;PORT_ON_THE_REMOTE/LOCAL_MACHINE&gt; &lt;USERNAME&gt;@&lt;REMOTE_IP&gt;
</code></pre>

<p>In our particular case, this looks like this:</p>

<pre><code class="language-bash">ssh -L 9200:127.0.0.1:9200  user@XX.XXX.XXX.XXX
</code></pre>

<p>This basically says: map my local port <code>9200</code> to a call to <code>127.0.0.1:9200</code> on the <code>XX.XXX.XXX.XXX</code> server. When you point your browser to <code>http://localhost:9200</code>, you should now see the familiar ElasticSearch output, even though, as before <code>XX.XXX.XXX.XXX:9200</code> returns nothing. You can let the above command run in the background and run as a daemon.</p>

<h2 id="conclusion">Conclusion</h2>

<p>These two steps are all you need, in order to keep enjoying the freedom of playing with ElasticSearch or MongoDB, but doing it in a fully secure manner. This recipe can be applied to just about any service. And you really don&rsquo;t need Docker even. The fact that I mentioned it in part one, is because it makes setting up easy, and also saves you from having to tinker with <code>iptables</code> yourselves.</p>

<p><strong>NOTE:</strong> Please, keep in mind that while running a SSH tunnel is just about perfect for testing and development purposes, it may not be an optimal solution for production. The reason for this is the latency caused by en/decrypting the data and shuffling it through the tunnel. It may become a bottleneck with many incoming requests running in parallel. I am yet to stress-test this setup and will share my observations in a further post. I will also share some more ideas on how to access an ElasticSearch instance securely, but also in a productive manner.</p>
]]></content>
        </item>
        
    </channel>
</rss>
